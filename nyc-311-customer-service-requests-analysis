import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from scipy.stats as chi2_contingency
%matplotlib inline

data = pd.read_csv("NYC311data.csv", parse_dates = ['Created Date','Closed Date','Resolution Action Updated Date'])
data.info()

def convert_lower(data):
    #convert string columns into lowercase to elimintae duplication
    data = data.applymap(lambda s:s.lower() if type(s) == str else s)
    return data

def drop_empty_columns(data):
    #drop the columns which are completly/almost empty
    na_col_count = data.isna().sum().where(lambda x:x>0).dropna()
    columns_to_drop = []
    for column in na_col_count.iteritems():
        if(column[1]/8275 >= 0.85):
            columns_to_drop.append(column[0])
    data.drop(columns_to_drop, axis=1, inplace=True)
    return data

def drop_unneccessary_columns(data):
    #drop the unneccesary columns
    cols = ['Descriptor','Location','Address Type','Facility Type','Agency','Agency Name','Community Board','Borough','Park Facility Name','Park Borough','School Region','School Code', 'School Name','School Number','School Phone Number','School Address','School City','School State','School Not Found','School Zip','X Coordinate (State Plane)','Y Coordinate (State Plane)']
    data.drop(cols,axis=1,inplace=True)
    return data

data = convert_lower(data)
data = drop_empty_columns(data)
data = drop_unneccessary_columns(data)
data.isna().sum().where(lambda x:x>0).dropna()


#dropping rows where closed and resolution date both are empty
data = data[~(data['Closed Date'].isna() & data['Resolution Action Updated Date'].isna())]

#updating closed date with resolution date where closed date is empty and vice versa
data.loc[data['Closed Date'].isna(),'Closed Date'] = data[data['Closed Date'].isna()]['Resolution Action Updated Date']
data.loc[data['Resolution Action Updated Date'].isna(),'Resolution Action Updated Date'] = data[data['Resolution Action Updated Date'].isna()]['Closed Date']
data.loc[data['Due Date'].isna(),'Due Date'] = data[data['Due Date'].isna()]['Closed Date']

#since now all rows have a completion date(closed/resolution) status of all complaints updated to close
data.loc[data.Status.isin(['open','assigned','draft']),'Status'] = 'closed'


#filling na values based on highest occuring value(mode)
#get mode values
city = data.City.mode()[0]
zipcode = data.loc[data.City == city,'Incident Zip'].mode()[0]
latitude = data.loc[data['Incident Zip'] == zipcode,'Latitude'].mode()[0]
longitude = data.loc[data['Incident Zip'] == zipcode,'Longitude'].mode()[0]
#fill zipcode,latitude and longitude with modevalues where city is empty and then update city value to mode value
data.loc[data.City.isna(),'Incident Zip'] = zipcode
data.loc[data.City.isna(),'City'] = city
data.loc[(data.City == city) & (data.Latitude.isna()==True),'Latitude'] = latitude
data.loc[(data.City == city) & (data.Longitude.isna()==True),'Longitude'] = longitude

data.isna().sum().where(lambda x:x>0).dropna()

data.dropna(inplace=True)
data.info()

#create new column request closing time
data.loc[:,'Request Closing Time(in hours)'] = (data['Closed Date'] - data['Created Date']).dt.total_seconds()



## BARPLOT

#total complaints based on category
plt.figure(figsize=(10,8))
plt.title('Complaints Types Distribution')
sns.countplot(y='Complaint Type', data=data)
plt.show()

#citywise complaint counts(total)
plt.figure(figsize=(15,12))
plt.title('Citywise total complaints')
sns.countplot(y='City',data=data)
plt.show()

top5cities = data['City'].value_counts().head(5).index.to_list()
dstop5 = data[data.City.isin(top5cities)]
#citywise complaint counts(typewise)
data1 = pd.crosstab(dstop5['City'],dstop5['Complaint Type'])

##citywise complaint counts(typewise)
data1.plot(kind='barh',stacked=True,figsize=(12,10))
plt.title('Categorywise Complaints per city')
plt.show()



## Hypothesis testing

* Whether the average response time across complaint types is similar or not (overall)
> When your experiment is trying to draw a comparison or find the difference between one categorical (with more than two categories) and another continuous variable, then you use the ANOVA (Analysis of Variance) test. 
* Are the type of complaint or service requested and location related?
> Complaint Type(CAT), Location(CAT) : Chi-Square Test

#new_ds = ds.dr
complaintTypes = data['Complaint Type'].unique()
for i in range(len(complaintTypes)):
    exec("c{} = data.loc[(data['Complaint Type'] == '{}'),'Request Closing Time(in hours)']".format(i+1,complaintTypes[i]))
    
#Chi square test: Complaint Type(CAT), Location(CAT) 
ctabDF = pd.crosstab(data['Complaint Type'],data['City'])

stat, p, dof, expected = chi2_contingency(ctabDF)
print('Chi-Square Statistic Value: ',stat)
print('p value: ',p)
print('degrees of freedom: ',dof)
